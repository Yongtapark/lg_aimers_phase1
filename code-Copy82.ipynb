{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "017e9265",
   "metadata": {},
   "source": [
    "# 제품 이상여부 판별 프로젝트\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdab431",
   "metadata": {},
   "source": [
    "## 1. 데이터 불러오기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8341e8",
   "metadata": {},
   "source": [
    "### 필수 라이브러리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01a09334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (3.6.1)\n",
      "Requirement already satisfied: catboost in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (1.2.5)\n",
      "Requirement already satisfied: xgboost in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (2.1.0)\n",
      "Collecting nodejs\n",
      "  Downloading nodejs-0.1.1.tar.gz (2.3 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: alembic>=1.5.0 in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from optuna) (1.13.2)\n",
      "Requirement already satisfied: numpy in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from optuna) (1.25.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from optuna) (2.0.32)\n",
      "Requirement already satisfied: PyYAML in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from optuna) (24.1)\n",
      "Requirement already satisfied: colorlog in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from optuna) (6.8.2)\n",
      "Requirement already satisfied: tqdm in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from optuna) (4.66.4)\n",
      "Requirement already satisfied: scipy in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from catboost) (1.14.0)\n",
      "Requirement already satisfied: matplotlib in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from catboost) (3.9.1)\n",
      "Requirement already satisfied: graphviz in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from catboost) (0.20.3)\n",
      "Requirement already satisfied: plotly in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from catboost) (5.23.0)\n",
      "Requirement already satisfied: pandas>=0.24 in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from catboost) (2.2.2)\n",
      "Requirement already satisfied: six in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from catboost) (1.16.0)\n",
      "Collecting optional-django==0.1.0\n",
      "  Downloading optional-django-0.1.0.tar.gz (9.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: Mako in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
      "Requirement already satisfied: typing-extensions>=4 in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib->catboost) (1.2.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib->catboost) (10.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib->catboost) (4.53.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib->catboost) (3.1.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib->catboost) (1.4.5)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from plotly->catboost) (8.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/yongtapark/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
      "Installing collected packages: optional-django, nodejs\n",
      "\u001b[33m  DEPRECATION: optional-django is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running setup.py install for optional-django ... \u001b[?25ldone\n",
      "\u001b[33m  DEPRECATION: nodejs is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[?25h  Running setup.py install for nodejs ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed nodejs-0.1.1 optional-django-0.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna catboost xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a315cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import catboost\n",
    "import lightgbm\n",
    "import xgboost\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from optuna.samplers import TPESampler\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d054e30",
   "metadata": {},
   "source": [
    "### 데이터 읽어오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c17bf0da-7dc1-4670-8702-3c204b2e2084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_datas(df):\n",
    "    \n",
    "    no_value_col = df.columns[df.nunique()==0] \n",
    "    df.drop(columns=no_value_col,inplace=True)\n",
    "\n",
    "    df.fillna(0,inplace=True)\n",
    "\n",
    "    # 중복값 가진 컬럼 제거\n",
    "    dfT= df.T\n",
    "    df_unique = dfT[~dfT.duplicated()]\n",
    "    filterd_df = df_unique.T\n",
    "    return filterd_df.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc0b4d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"data\"\n",
    "RANDOM_STATE = 110\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv(os.path.join(ROOT_DIR, \"train.csv\"))\n",
    "test_data = pd.read_csv(os.path.join(ROOT_DIR, \"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be216ebb-de8e-4f99-853b-c83d3f031076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vl/r365j2vx2wv3v02bqbw29rk80000gn/T/ipykernel_13087/2285825296.py:6: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.fillna(0,inplace=True)\n"
     ]
    }
   ],
   "source": [
    "## 테스트 , 훈련 데이터 통합 전처리\n",
    "\n",
    "# 데이터에 라벨 추가 (train: 0, test: 1)\n",
    "train_data['dataset_label'] = 0\n",
    "test_data['dataset_label'] = 1\n",
    "\n",
    "train_y = train_data['target']\n",
    "test_set_id = test_data['Set ID']\n",
    "\n",
    "train_x = train_data.drop(columns='target')\n",
    "test_x = test_data.drop(columns='Set ID')\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터를 합침\n",
    "combined_data = pd.concat([train_x, test_x], axis=0)\n",
    "\n",
    "error_cols = ['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam','HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1','HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2']\n",
    "combined_data.loc[:, error_cols] = combined_data.loc[:, error_cols].replace('OK', np.nan).astype('float')\n",
    "combined_data = clean_datas(combined_data)\n",
    "\n",
    "cat_features = list(combined_data.select_dtypes(include=['string','object']).columns)\n",
    "# 결측값을 'Missing'으로 대체\n",
    "combined_data[cat_features] = combined_data[cat_features].fillna('NA').astype('category')\n",
    "combined_data_gx = pd.get_dummies(combined_data, columns=cat_features)\n",
    "\n",
    "\n",
    "# 다시 훈련 데이터와 테스트 데이터로 분리\n",
    "train_x = combined_data[combined_data['dataset_label'] == 0].drop(columns=['dataset_label'])\n",
    "test_x = combined_data[combined_data['dataset_label'] == 1].drop(columns=['dataset_label'])\n",
    "\n",
    "train_x_gx = combined_data_gx[combined_data_gx['dataset_label'] == 0].drop(columns=['dataset_label'])\n",
    "test_x_gx = combined_data_gx[combined_data_gx['dataset_label'] == 1].drop(columns=['dataset_label'])\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "train_y_encoded = le.fit_transform(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07b837fd-743c-4563-baef-518fab8b49c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m qual_col \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_x\u001b[49m\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m qual_col:\n\u001b[1;32m      4\u001b[0m     lec \u001b[38;5;241m=\u001b[39m LabelEncoder()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "qual_col = train_x.select_dtypes(include='object')\n",
    "\n",
    "for i in qual_col:\n",
    "    lec = LabelEncoder()\n",
    "    lec = lec.fit(train_x[i])\n",
    "    train_x[i] = lec.transform(train_x[i])\n",
    "    \n",
    "    for label in np.unique(test_x[i]): \n",
    "        if label not in lec.classes_: \n",
    "            lec.classes_ = np.append(lec.classes_, label)\n",
    "    test_x[i] = lec.transform(test_x[i]) \n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0eee1eda-a835-4c58-a1e7-29a3775efc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치 처리 \n",
    "def clean_column(col, reference_col=None):\n",
    "    if reference_col is None:\n",
    "        reference_col = col\n",
    "        \n",
    "    z_scores = (reference_col - reference_col.mean()) / reference_col.std()\n",
    "    col_cleaned = col.mask(abs(z_scores) > 3)\n",
    "\n",
    "    most_frequent_value = reference_col.mode()[0]\n",
    "\n",
    "    col_filled = col_cleaned.fillna(most_frequent_value)\n",
    "\n",
    "    return col_filled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "40dc14cc-e1e5-43a8-bfcf-ec831da894c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 이상치 처리 train, test에 동일하게 적용\n",
    "train_x = train_x.apply(clean_column)\n",
    "test_x = test_x.apply(lambda col: clean_column(col, reference_col=train_x[col.name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ecfa9b",
   "metadata": {},
   "source": [
    "## 3. 모델 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53844b04-4746-4614-acc8-b9bc793fe691",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# 클래스 가중치 계산\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_y), y=train_y)\n",
    "class_weights_non_cat = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caf257b",
   "metadata": {},
   "source": [
    "### 모델 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "966bb8a0-d603-472b-81e8-c4224c710fb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6900524\ttotal: 14.3ms\tremaining: 7.13s\n",
      "100:\tlearn: 0.6194459\ttotal: 1.09s\tremaining: 4.33s\n",
      "200:\tlearn: 0.5930066\ttotal: 2.18s\tremaining: 3.24s\n",
      "300:\tlearn: 0.5505096\ttotal: 3.25s\tremaining: 2.15s\n",
      "400:\tlearn: 0.5136037\ttotal: 4.35s\tremaining: 1.07s\n",
      "499:\tlearn: 0.4826905\ttotal: 5.43s\tremaining: 0us\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 30524, number of negative: 1880\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002578 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4254\n",
      "[LightGBM] [Info] Number of data points in the train set: 32404, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499993 -> initscore=-0.000026\n",
      "[LightGBM] [Info] Start training from score -0.000026\n",
      "0:\tlearn: 0.6906761\ttotal: 11.4ms\tremaining: 5.67s\n",
      "100:\tlearn: 0.6228436\ttotal: 1.09s\tremaining: 4.33s\n",
      "200:\tlearn: 0.5971742\ttotal: 2.17s\tremaining: 3.23s\n",
      "300:\tlearn: 0.5512717\ttotal: 3.27s\tremaining: 2.16s\n",
      "400:\tlearn: 0.5155287\ttotal: 4.34s\tremaining: 1.07s\n",
      "499:\tlearn: 0.4856445\ttotal: 5.42s\tremaining: 0us\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 30525, number of negative: 1880\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002682 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4228\n",
      "[LightGBM] [Info] Number of data points in the train set: 32405, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500002 -> initscore=0.000007\n",
      "[LightGBM] [Info] Start training from score 0.000007\n",
      "0:\tlearn: 0.6899439\ttotal: 11.5ms\tremaining: 5.74s\n",
      "100:\tlearn: 0.6242476\ttotal: 1.07s\tremaining: 4.25s\n",
      "200:\tlearn: 0.5955244\ttotal: 2.19s\tremaining: 3.26s\n",
      "300:\tlearn: 0.5524540\ttotal: 3.3s\tremaining: 2.18s\n",
      "400:\tlearn: 0.5173913\ttotal: 4.39s\tremaining: 1.08s\n",
      "499:\tlearn: 0.4856237\ttotal: 5.48s\tremaining: 0us\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 30525, number of negative: 1880\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003577 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4237\n",
      "[LightGBM] [Info] Number of data points in the train set: 32405, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500002 -> initscore=0.000007\n",
      "[LightGBM] [Info] Start training from score 0.000007\n",
      "0:\tlearn: 0.6902870\ttotal: 11.5ms\tremaining: 5.74s\n",
      "100:\tlearn: 0.6211030\ttotal: 1.08s\tremaining: 4.28s\n",
      "200:\tlearn: 0.5940768\ttotal: 2.18s\tremaining: 3.24s\n",
      "300:\tlearn: 0.5505625\ttotal: 3.27s\tremaining: 2.16s\n",
      "400:\tlearn: 0.5127939\ttotal: 4.37s\tremaining: 1.08s\n",
      "499:\tlearn: 0.4822722\ttotal: 5.46s\tremaining: 0us\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 30525, number of negative: 1880\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4245\n",
      "[LightGBM] [Info] Number of data points in the train set: 32405, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500002 -> initscore=0.000007\n",
      "[LightGBM] [Info] Start training from score 0.000007\n",
      "0:\tlearn: 0.6901305\ttotal: 11.4ms\tremaining: 5.67s\n",
      "100:\tlearn: 0.6223490\ttotal: 1.1s\tremaining: 4.33s\n",
      "200:\tlearn: 0.5936238\ttotal: 2.22s\tremaining: 3.3s\n",
      "300:\tlearn: 0.5505871\ttotal: 3.31s\tremaining: 2.19s\n",
      "400:\tlearn: 0.5140300\ttotal: 4.44s\tremaining: 1.09s\n",
      "499:\tlearn: 0.4849052\ttotal: 5.56s\tremaining: 0us\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 30525, number of negative: 1880\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4254\n",
      "[LightGBM] [Info] Number of data points in the train set: 32405, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500002 -> initscore=0.000007\n",
      "[LightGBM] [Info] Start training from score 0.000007\n",
      "Stratified K-Fold Cross-Validation F1 Scores: [0.53985342 0.5406242  0.55001144 0.54368097 0.54724662]\n",
      "Mean CV F1 Score: 0.5442833331356095\n",
      "0:\tlearn: 0.6895332\ttotal: 11.7ms\tremaining: 5.86s\n",
      "100:\tlearn: 0.6221544\ttotal: 1.25s\tremaining: 4.92s\n",
      "200:\tlearn: 0.6016195\ttotal: 2.5s\tremaining: 3.72s\n",
      "300:\tlearn: 0.5650044\ttotal: 3.8s\tremaining: 2.51s\n",
      "400:\tlearn: 0.5335699\ttotal: 5.12s\tremaining: 1.26s\n",
      "499:\tlearn: 0.5072655\ttotal: 6.36s\tremaining: 0us\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 38156, number of negative: 2350\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4247\n",
      "[LightGBM] [Info] Number of data points in the train set: 40506, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training F1 Score: 0.640110139319426\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# 단순화된 모델을 사용한 앙상블\n",
    "cat_params = {\n",
    "    'iterations': 500, \n",
    "    #'depth': 7,\n",
    "    'learning_rate': 0.05, \n",
    "    'random_state': 42,\n",
    "    'verbose': 100\n",
    "}\n",
    "\n",
    "lgbm_params = {\n",
    "    'num_leaves': 31, \n",
    "    #'max_depth': -1, \n",
    "    'learning_rate': 0.05, \n",
    "    'n_estimators': 300, \n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# CatBoost 모델\n",
    "cat_model = CatBoostClassifier(class_weights=class_weights_non_cat,cat_features=cat_features,**cat_params)\n",
    "\n",
    "# LightGBM 모델\n",
    "lgbm_model = LGBMClassifier(class_weight=class_weights_non_cat,**lgbm_params)\n",
    "\n",
    "# Voting Classifier 생성 (간단한 앙상블)\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('cat', cat_model),\n",
    "        ('lgbm', lgbm_model),\n",
    "        #('rf',lr_model)\n",
    "    ],\n",
    "    voting='soft'  # 'soft'는 각 모델의 확률을 평균, 'hard'는 다수결\n",
    ")\n",
    "\n",
    "# Stratified K-Fold 교차 검증\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "f1_scorer = make_scorer(f1_score, average='macro')\n",
    "cv_scores = cross_val_score(voting_clf, train_x, train_y_encoded, cv=skf, scoring=f1_scorer)\n",
    "\n",
    "# 교차 검증 점수 출력\n",
    "print(f\"Stratified K-Fold Cross-Validation F1 Scores: {cv_scores}\")\n",
    "print(f\"Mean CV F1 Score: {np.mean(cv_scores)}\")\n",
    "\n",
    "# 훈련 데이터에서 모델을 학습하고 F1 스코어 평가\n",
    "voting_clf.fit(train_x, train_y_encoded)\n",
    "train_pred = voting_clf.predict(train_x)\n",
    "train_f1 = f1_score(train_y_encoded, train_pred, average='macro')\n",
    "\n",
    "print(f\"Training F1 Score: {train_f1}\")\n",
    "\n",
    "# 최종 테스트 데이터 예측\n",
    "voting_pred = voting_clf.predict(test_x)\n",
    "voting_pred = le.inverse_transform(voting_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacd5ed8",
   "metadata": {},
   "source": [
    "### 모델 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fadbee0-2314-46e7-bb6b-3c6790c16aa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6900524\ttotal: 13.9ms\tremaining: 6.92s\n",
      "100:\tlearn: 0.6194459\ttotal: 1.11s\tremaining: 4.4s\n",
      "200:\tlearn: 0.5930066\ttotal: 2.2s\tremaining: 3.28s\n",
      "300:\tlearn: 0.5505096\ttotal: 3.3s\tremaining: 2.18s\n",
      "400:\tlearn: 0.5136037\ttotal: 4.38s\tremaining: 1.08s\n",
      "499:\tlearn: 0.4826905\ttotal: 5.47s\tremaining: 0us\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 30524, number of negative: 1880\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4254\n",
      "[LightGBM] [Info] Number of data points in the train set: 32404, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499993 -> initscore=-0.000026\n",
      "[LightGBM] [Info] Start training from score -0.000026\n",
      "0:\tlearn: 0.6902073\ttotal: 11.2ms\tremaining: 5.58s\n",
      "100:\tlearn: 0.6256845\ttotal: 973ms\tremaining: 3.84s\n",
      "200:\tlearn: 0.5955001\ttotal: 1.95s\tremaining: 2.9s\n",
      "300:\tlearn: 0.5420070\ttotal: 2.93s\tremaining: 1.94s\n",
      "400:\tlearn: 0.5001298\ttotal: 3.91s\tremaining: 966ms\n",
      "499:\tlearn: 0.4663136\ttotal: 4.88s\tremaining: 0us\n",
      "0:\tlearn: 0.6903826\ttotal: 9.82ms\tremaining: 4.9s\n",
      "100:\tlearn: 0.6165753\ttotal: 962ms\tremaining: 3.8s\n",
      "200:\tlearn: 0.5826931\ttotal: 1.94s\tremaining: 2.88s\n",
      "300:\tlearn: 0.5308053\ttotal: 2.92s\tremaining: 1.93s\n",
      "400:\tlearn: 0.4884623\ttotal: 3.94s\tremaining: 974ms\n",
      "499:\tlearn: 0.4523113\ttotal: 4.93s\tremaining: 0us\n",
      "0:\tlearn: 0.6905560\ttotal: 11.2ms\tremaining: 5.58s\n",
      "100:\tlearn: 0.6182727\ttotal: 1s\tremaining: 3.96s\n",
      "200:\tlearn: 0.5837398\ttotal: 1.99s\tremaining: 2.97s\n",
      "300:\tlearn: 0.5347685\ttotal: 2.96s\tremaining: 1.96s\n",
      "400:\tlearn: 0.4935659\ttotal: 3.94s\tremaining: 972ms\n",
      "499:\tlearn: 0.4581216\ttotal: 4.91s\tremaining: 0us\n",
      "0:\tlearn: 0.6905099\ttotal: 10.1ms\tremaining: 5.03s\n",
      "100:\tlearn: 0.6170512\ttotal: 971ms\tremaining: 3.84s\n",
      "200:\tlearn: 0.5831721\ttotal: 1.95s\tremaining: 2.9s\n",
      "300:\tlearn: 0.5345739\ttotal: 2.93s\tremaining: 1.94s\n",
      "400:\tlearn: 0.4920298\ttotal: 3.92s\tremaining: 968ms\n",
      "499:\tlearn: 0.4564486\ttotal: 4.89s\tremaining: 0us\n",
      "0:\tlearn: 0.6902108\ttotal: 9.77ms\tremaining: 4.88s\n",
      "100:\tlearn: 0.6159419\ttotal: 989ms\tremaining: 3.91s\n",
      "200:\tlearn: 0.5842238\ttotal: 1.96s\tremaining: 2.92s\n",
      "300:\tlearn: 0.5342997\ttotal: 2.95s\tremaining: 1.95s\n",
      "400:\tlearn: 0.4919470\ttotal: 3.93s\tremaining: 970ms\n",
      "499:\tlearn: 0.4597114\ttotal: 4.9s\tremaining: 0us\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 24419, number of negative: 1504\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002352 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4205\n",
      "[LightGBM] [Info] Number of data points in the train set: 25923, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000034\n",
      "[LightGBM] [Info] Start training from score -0.000034\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 24419, number of negative: 1504\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4207\n",
      "[LightGBM] [Info] Number of data points in the train set: 25923, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000034\n",
      "[LightGBM] [Info] Start training from score -0.000034\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 24419, number of negative: 1504\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4208\n",
      "[LightGBM] [Info] Number of data points in the train set: 25923, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000034\n",
      "[LightGBM] [Info] Start training from score -0.000034\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 24419, number of negative: 1504\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002421 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4203\n",
      "[LightGBM] [Info] Number of data points in the train set: 25923, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499991 -> initscore=-0.000034\n",
      "[LightGBM] [Info] Start training from score -0.000034\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 24420, number of negative: 1504\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002314 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4199\n",
      "[LightGBM] [Info] Number of data points in the train set: 25924, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500002 -> initscore=0.000007\n",
      "[LightGBM] [Info] Start training from score 0.000007\n",
      "0:\tlearn: 0.6906761\ttotal: 11.7ms\tremaining: 5.85s\n",
      "100:\tlearn: 0.6228436\ttotal: 1.08s\tremaining: 4.28s\n",
      "200:\tlearn: 0.5971742\ttotal: 2.16s\tremaining: 3.21s\n",
      "300:\tlearn: 0.5512717\ttotal: 3.29s\tremaining: 2.17s\n",
      "400:\tlearn: 0.5155287\ttotal: 4.42s\tremaining: 1.09s\n",
      "499:\tlearn: 0.4856445\ttotal: 5.52s\tremaining: 0us\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 30525, number of negative: 1880\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002897 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4228\n",
      "[LightGBM] [Info] Number of data points in the train set: 32405, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500002 -> initscore=0.000007\n",
      "[LightGBM] [Info] Start training from score 0.000007\n",
      "0:\tlearn: 0.6901318\ttotal: 10.6ms\tremaining: 5.28s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# 단순화된 모델을 사용한 앙상블\n",
    "cat_params = {\n",
    "    'iterations': 500, \n",
    "    'learning_rate': 0.05, \n",
    "    'random_state': 42,\n",
    "    'verbose': 100\n",
    "}\n",
    "\n",
    "lgbm_params = {\n",
    "    'num_leaves': 31, \n",
    "    'learning_rate': 0.05, \n",
    "    'n_estimators': 300, \n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# CatBoost 모델\n",
    "cat_model = CatBoostClassifier(class_weights=class_weights_non_cat,cat_features=cat_features,**cat_params)\n",
    "\n",
    "# LightGBM 모델\n",
    "lgbm_model = LGBMClassifier(class_weight=class_weights_non_cat,**lgbm_params)\n",
    "\n",
    "# 메타모델 (XGBoost)\n",
    "meta_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# 스태킹 앙상블 생성\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('cat', cat_model),\n",
    "        ('lgbm', lgbm_model)\n",
    "    ],\n",
    "    final_estimator=meta_model,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Stratified K-Fold 교차 검증\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "f1_scorer = make_scorer(f1_score, average='macro')\n",
    "cv_scores = cross_val_score(stacking_clf, train_x, train_y_encoded, cv=skf, scoring=f1_scorer)\n",
    "\n",
    "# 교차 검증 점수 출력\n",
    "print(f\"Stratified K-Fold Cross-Validation F1 Scores: {cv_scores}\")\n",
    "print(f\"Mean CV F1 Score: {np.mean(cv_scores)}\")\n",
    "\n",
    "# 훈련 데이터에서 모델을 학습하고 F1 스코어 평가\n",
    "stacking_clf.fit(train_x, train_y_encoded)\n",
    "train_pred = stacking_clf.predict(train_x)\n",
    "train_f1 = f1_score(train_y_encoded, train_pred, average='macro')\n",
    "\n",
    "print(f\"Training F1 Score: {train_f1}\")\n",
    "\n",
    "# 최종 테스트 데이터 예측\n",
    "stacking_pred = stacking_clf.predict(test_x)\n",
    "stacking_pred = le.inverse_transform(stacking_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf8300",
   "metadata": {},
   "source": [
    "## 4. 제출하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f18e6a",
   "metadata": {},
   "source": [
    "### 제출 파일 작성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35c751c6-e7dc-4df0-a065-f605e7ebf2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Counter({'Normal': 13662, 'AbNormal': 3699})\n",
      "Normal to AbNormal: 2902\n",
      "AbNormal to Normal: 41\n",
      "Same classification: 14418\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 기존 결과와 새로운 결과 로드\n",
    "original_df = pd.read_csv('submission_best.csv')\n",
    "new_df = original_df.copy()\n",
    "new_df['target'] = voting_pred\n",
    "print(type(new_df))\n",
    "\n",
    "original_labels = original_df['target']\n",
    "new_labels = new_df['target']\n",
    "\n",
    "# 정상에서 비정상으로 변한 데이터\n",
    "normal_to_abnormal = original_df[(original_labels == 'Normal') & (new_labels == 'AbNormal')]\n",
    "\n",
    "# 비정상에서 정상으로 변한 데이터\n",
    "abnormal_to_normal = original_df[(original_labels == 'AbNormal') & (new_labels == 'Normal')]\n",
    "\n",
    "# 동일한 상태를 유지한 데이터\n",
    "same_classification = original_df[original_labels == new_labels]\n",
    "\n",
    "# 변화한 데이터 개수 확인\n",
    "from collections import Counter\n",
    "print(Counter(list(new_labels)))\n",
    "print(f\"Normal to AbNormal: {len(normal_to_abnormal)}\")\n",
    "print(f\"AbNormal to Normal: {len(abnormal_to_normal)}\")\n",
    "print(f\"Same classification: {len(same_classification)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3128a458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(\"submission.csv\")\n",
    "df_sub[\"target\"] = new_labels\n",
    "\n",
    "# 제출 파일 저장\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7867ce",
   "metadata": {},
   "source": [
    "**우측 상단의 제출 버튼을 클릭해 결과를 확인하세요**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
